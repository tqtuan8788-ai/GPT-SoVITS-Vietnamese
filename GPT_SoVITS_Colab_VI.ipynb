{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fff13a",
   "metadata": {},
   "source": [
    "# GPT-SoVITS v2 - Vietnamese Voice Cloning Pipeline (Final Fix v4)\n",
    "\n",
    "ÄÃ¢y lÃ  notebook tá»‘i Æ°u hÃ³a Ä‘á»ƒ huáº¥n luyá»‡n (train) vÃ  sá»­ dá»¥ng (inference) mÃ´ hÃ¬nh GPT-SoVITS v2 trÃªn Google Colab.\n",
    "\n",
    "### ðŸŒŸ TÃ­nh nÄƒng má»›i:\n",
    "- **Fix lá»—i WebUI ASR**: ThÃªm bÆ°á»›c cháº¡y ASR Tiáº¿ng Viá»‡t thá»§ cÃ´ng ngay trong Notebook (khÃ´ng cáº§n lÃ m trÃªn WebUI).\n",
    "- **Fix lá»—i HFValidationError**: Tá»± Ä‘á»™ng patch file config dÃ¹ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i.\n",
    "- **TÃ­ch há»£p UVR5**: Tá»± Ä‘á»™ng tÃ¡ch nháº¡c ná»n.\n",
    "\n",
    "### ðŸš€ Quy trÃ¬nh thá»±c hiá»‡n:\n",
    "1. **CÃ i Ä‘áº·t mÃ´i trÆ°á»ng**.\n",
    "2. **Táº£i Dá»¯ liá»‡u & LÃ m sáº¡ch (UVR5)**.\n",
    "3. **WebUI (BÆ°á»›c 1)**: Chá»‰ dÃ¹ng Ä‘á»ƒ Cáº¯t Ã¢m thanh (Slicing).\n",
    "4. **ASR Thá»§ cÃ´ng**: Quay láº¡i Ä‘Ã¢y cháº¡y Cell táº¡o phá»¥ Ä‘á» tiáº¿ng Viá»‡t.\n",
    "5. **WebUI (BÆ°á»›c 2)**: Quay láº¡i WebUI Ä‘á»ƒ Train vÃ  Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f6a46",
   "metadata": {
    "id": "step1_install"
   },
   "outputs": [],
   "source": [
    "# @title 1. CÃ i Ä‘áº·t MÃ´i trÆ°á»ng, UVR5 & GPT-SoVITS (Fix Download)\n",
    "# @markdown Cháº¡y cell nÃ y Ä‘á»ƒ cÃ i Ä‘áº·t toÃ n bá»™ thÆ° viá»‡n vÃ  táº£i model thá»§ cÃ´ng.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 1. Clone Repo GPT-SoVITS\n",
    "if not os.path.exists(\"GPT-SoVITS-Vietnamese\"):\n",
    "    print(\"Dang clone repository GPT-SoVITS...\")\n",
    "    !git clone https://github.com/tqtuan8788-ai/GPT-SoVITS-Vietnamese.git\n",
    "    %cd GPT-SoVITS-Vietnamese\n",
    "else:\n",
    "    %cd GPT-SoVITS-Vietnamese\n",
    "    !git pull\n",
    "\n",
    "# 2. Install Dependencies\n",
    "print(\"Dang cai dat thu vien chinh...\")\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q openai-whisper\n",
    "\n",
    "# 3. CÃ i Ä‘áº·t UVR5 (Ä‘á»ƒ tÃ¡ch nháº¡c/vocal)\n",
    "print(\"Dang cai dat UVR5 va cac cong cu xu ly am thanh...\")\n",
    "!pip install -q soundfile librosa noise-reduction onnxruntime-gpu\n",
    "# Táº£i model UVR5\n",
    "!mkdir -p uvr5_weights\n",
    "!wget -nc -P uvr5_weights https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/uvr5_weights/HP5_only_main_vocal.pth\n",
    "!wget -nc -P uvr5_weights https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/uvr5_weights/onnx_dereverb_By_FoxJoy/vocals.onnx\n",
    "\n",
    "# 4. Táº£i Pretrained Models THá»¦ CÃ”NG (Fix lá»—i script download)\n",
    "print(\"Dang tai pre-trained models thu cong tu Hugging Face...\")\n",
    "\n",
    "# Táº¡o Ä‘Ãºng cáº¥u trÃºc thÆ° má»¥c mÃ  code yÃªu cáº§u: GPT_SoVITS/pretrained_models/...\n",
    "base_model_dir = \"GPT_SoVITS/pretrained_models\"\n",
    "os.makedirs(base_model_dir, exist_ok=True)\n",
    "os.makedirs(f\"{base_model_dir}/chinese-roberta-wwm-ext-large\", exist_ok=True)\n",
    "os.makedirs(f\"{base_model_dir}/chinese-hubert-base\", exist_ok=True)\n",
    "\n",
    "# Táº£i Chinese-Roberta (Báº¯t buá»™c cho Text encoding)\n",
    "!wget -nc -P {base_model_dir}/chinese-roberta-wwm-ext-large https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/config.json\n",
    "!wget -nc -P {base_model_dir}/chinese-roberta-wwm-ext-large https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/pytorch_model.bin\n",
    "!wget -nc -P {base_model_dir}/chinese-roberta-wwm-ext-large https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/tokenizer.json\n",
    "\n",
    "# Táº£i Chinese-Hubert (Báº¯t buá»™c cho Audio encoding)\n",
    "!wget -nc -P {base_model_dir}/chinese-hubert-base https://huggingface.co/TencentGameMate/chinese-hubert-base/resolve/main/config.json\n",
    "!wget -nc -P {base_model_dir}/chinese-hubert-base https://huggingface.co/TencentGameMate/chinese-hubert-base/resolve/main/pytorch_model.bin\n",
    "!wget -nc -P {base_model_dir}/chinese-hubert-base https://huggingface.co/TencentGameMate/chinese-hubert-base/resolve/main/preprocessor_config.json\n",
    "\n",
    "# Táº£i GPT-SoVITS v2 Pretrained Models (s2G, s2D, s1bert)\n",
    "!wget -nc -P {base_model_dir} https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/s2G488k.pth\n",
    "!wget -nc -P {base_model_dir} https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/s2D488k.pth\n",
    "!wget -nc -P {base_model_dir} https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/s1bert25Hz-2kh-longer-epoch=68e-step=50232.ckpt\n",
    "\n",
    "# Táº£i G2PW Model (Cho tiáº¿ng Trung, nhÆ°ng script váº«n check)\n",
    "!wget -nc -P GPT_SoVITS/text/G2PWModel https://paddlespeech.bj.bcebos.com/Parakeet/op_g2p_model/G2PWModel/G2PWModel_1.1.zip\n",
    "!unzip -o -q GPT_SoVITS/text/G2PWModel/G2PWModel_1.1.zip -d GPT_SoVITS/text/G2PWModel\n",
    "\n",
    "clear_output()\n",
    "print(\"âœ… CÃ i Ä‘áº·t hoÃ n táº¥t! ÄÃ£ táº£i Ä‘á»§ models thá»§ cÃ´ng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3_slice_asr",
   "metadata": {
    "id": "step3_slice_asr"
   },
   "outputs": [],
   "source": [
    "# @title 3. Cáº¯t Audio + Táº¡o Phá»¥ Äá» Tiáº¿ng Viá»‡t (ASR)\n",
    "# @markdown Cháº¡y cell nÃ y Ä‘á»ƒ cáº¯t audio thÃ nh cÃ¡c Ä‘oáº¡n ngáº¯n vÃ  táº¡o file .list tiáº¿ng Viá»‡t.\n",
    "# @markdown **LÆ°u Ã½:** Cell nÃ y sáº½ cháº¡y xong rá»“i dá»«ng, khÃ´ng cháº¡y liÃªn tá»¥c.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Äáº£m báº£o Ä‘ang á»Ÿ Ä‘Ãºng thÆ° má»¥c repo\n",
    "%cd /content/GPT-SoVITS-Vietnamese\n",
    "\n",
    "# CÃ i Ä‘áº·t thÃªm dependencies cáº§n thiáº¿t\n",
    "!pip install -q faster-whisper\n",
    "\n",
    "# Set PYTHONPATH Ä‘á»ƒ scripts tÃ¬m Ä‘Æ°á»£c modules\n",
    "import sys\n",
    "sys.path.insert(0, '/content/GPT-SoVITS-Vietnamese')\n",
    "os.environ['PYTHONPATH'] = '/content/GPT-SoVITS-Vietnamese'\n",
    "\n",
    "# âš ï¸ Äiá»n tÃªn experiment cá»§a báº¡n\n",
    "exp_name = \"Giong_Doc_Sach_01\" # @param {type:\"string\"}\n",
    "input_audio_folder = f\"/content/dataset/{exp_name}\"\n",
    "sliced_output_folder = \"output/slicer_opt\"\n",
    "asr_output_file = f\"output/{exp_name}.list\"\n",
    "\n",
    "os.makedirs(sliced_output_folder, exist_ok=True)\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# ========== BÆ¯á»šC 1: Cáº®T AUDIO ==========\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”ª BÆ¯á»šC 1: Cáº®T AUDIO THÃ€NH CÃC ÄOáº N NGáº®N\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“‚ Input: {input_audio_folder}\")\n",
    "print(f\"ðŸ“‚ Output: {sliced_output_folder}\")\n",
    "\n",
    "# Cháº¡y slice audio vá»›i subprocess\n",
    "slice_cmd = f'python tools/slice_audio.py \"{input_audio_folder}\" \"{sliced_output_folder}\" -34 4000 300 10 500 0.9 0.25 0 1'\n",
    "print(f\"Running: {slice_cmd}\")\n",
    "os.system(slice_cmd)\n",
    "\n",
    "# Äáº¿m file Ä‘Ã£ cáº¯t\n",
    "if os.path.exists(sliced_output_folder):\n",
    "    sliced_files = [f for f in os.listdir(sliced_output_folder) if f.endswith((\".wav\", \".mp3\", \".flac\"))]\n",
    "    print(f\"\\nâœ… ÄÃ£ cáº¯t xong! Tá»•ng cá»™ng {len(sliced_files)} file audio.\\n\")\n",
    "else:\n",
    "    sliced_files = []\n",
    "    print(\"\\nâš ï¸ ThÆ° má»¥c output chÆ°a cÃ³ file.\\n\")\n",
    "\n",
    "# ========== BÆ¯á»šC 2: ASR TIáº¾NG VIá»†T ==========\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸŽ¤ BÆ¯á»šC 2: NHáº¬N Dáº NG GIá»ŒNG NÃ“I (ASR) TIáº¾NG VIá»†T\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸ“‚ Input: {sliced_output_folder}\")\n",
    "print(f\"ðŸ“„ Output: {asr_output_file}\")\n",
    "print(\"â³ Äang cháº¡y Whisper large-v3... Vui lÃ²ng chá» 2-10 phÃºt.\\n\")\n",
    "\n",
    "# Cháº¡y ASR vá»›i subprocess - dÃ¹ng os.system Ä‘á»ƒ biáº¿n Ä‘Æ°á»£c thay tháº¿ Ä‘Ãºng\n",
    "asr_cmd = f'python tools/asr/fasterwhisper_asr.py -i \"{sliced_output_folder}\" -o \"{asr_output_file}\" -s large-v3 -l vi -p float32'\n",
    "print(f\"Running: {asr_cmd}\")\n",
    "os.system(asr_cmd)\n",
    "\n",
    "# Kiá»ƒm tra file .list Ä‘Ã£ táº¡o\n",
    "if os.path.exists(asr_output_file):\n",
    "    with open(asr_output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"\\nâœ… ÄÃƒ Táº O FILE .LIST THÃ€NH CÃ”NG!\")\n",
    "    print(f\"ðŸ“„ ÄÆ°á»ng dáº«n: {os.path.abspath(asr_output_file)}\")\n",
    "    print(f\"ðŸ“ Tá»•ng sá»‘ dÃ²ng: {len(lines)}\")\n",
    "    print(f\"\\nðŸ“‹ 5 dÃ²ng Ä‘áº§u tiÃªn:\")\n",
    "    for idx, line in enumerate(lines[:5]):\n",
    "        print(f\"  {idx+1}. {line.strip()[:80]}...\")\n",
    "else:\n",
    "    print(\"âŒ Lá»–I: KhÃ´ng táº¡o Ä‘Æ°á»£c file .list!\")\n",
    "    print(f\"   Kiá»ƒm tra: ThÆ° má»¥c input cÃ³ file audio khÃ´ng?\")\n",
    "    print(f\"   Input folder: {input_audio_folder}\")\n",
    "    if os.path.exists(input_audio_folder):\n",
    "        print(f\"   Files: {os.listdir(input_audio_folder)[:5]}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ThÆ° má»¥c input khÃ´ng tá»“n táº¡i!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… HOÃ€N Táº¤T! Tiáº¿p tá»¥c cháº¡y Cell 4 Ä‘á»ƒ má»Ÿ WebUI.\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ‘‰ Khi vÃ o WebUI, hÃ£y Ä‘iá»n Ä‘Æ°á»ng dáº«n nÃ y vÃ o Ã´ Text Label File:\")\n",
    "print(f\"   {os.path.abspath(asr_output_file)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4_webui",
   "metadata": {
    "id": "step4_webui"
   },
   "outputs": [],
   "source": [
    "# @title 4. Khá»Ÿi Ä‘á»™ng WebUI (Cháº¡y sau cÃ¹ng - Cháº¡y liÃªn tá»¥c)\n",
    "# @markdown **QUAN TRá»ŒNG:** Cell nÃ y sáº½ cháº¡y liÃªn tá»¥c Ä‘á»ƒ duy trÃ¬ WebUI.\n",
    "# @markdown Báº¡n sáº½ lÃ m viá»‡c trÃªn WebUI: Format data -> Train -> Inference\n",
    "\n",
    "import os\n",
    "\n",
    "# Äáº£m báº£o Ä‘ang á»Ÿ Ä‘Ãºng thÆ° má»¥c repo\n",
    "%cd /content/GPT-SoVITS-Vietnamese\n",
    "\n",
    "# Patch config.py\n",
    "cwd = os.getcwd()\n",
    "possible_bert_paths = [\n",
    "    os.path.join(cwd, \"GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\"),\n",
    "    os.path.join(cwd, \"pretrained_models/chinese-roberta-wwm-ext-large\"),\n",
    "    \"/content/GPT-SoVITS/GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\"\n",
    "]\n",
    "found_bert = next((p for p in possible_bert_paths if os.path.exists(p)), possible_bert_paths[0])\n",
    "found_hubert = found_bert.replace(\"chinese-roberta-wwm-ext-large\", \"chinese-hubert-base\")\n",
    "\n",
    "os.environ[\"cnhubert_base_path\"] = found_hubert\n",
    "os.environ[\"bert_path\"] = found_bert\n",
    "os.environ[\"colab_active\"] = \"1\"\n",
    "os.environ[\"is_share\"] = \"True\"\n",
    "\n",
    "print(\"ðŸš€ Äang khá»Ÿi Ä‘á»™ng WebUI...\")\n",
    "print(\"â³ Chá» khoáº£ng 1-2 phÃºt Ä‘á»ƒ WebUI load xong.\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“‹ HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG WEBUI:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Tab 1 -> 1A: Äiá»n tÃªn experiment vÃ  Ä‘Æ°á»ng dáº«n file .list\")\n",
    "print(\"2. Báº¥m nÃºt Format data\")\n",
    "print(\"3. Tab 1 -> 1B: Báº¥m Train SoVITS, sau Ä‘Ã³ Train GPT\")\n",
    "print(\"4. Tab 1 -> 1C: Chá»n model vÃ  táº¡o giá»ng nÃ³i\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "!sed -i \"s/is_share = False/is_share = True/g\" config.py\n",
    "!python webui.py --share --i18n_dir ./i18n/vi_VN.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd94bf",
   "metadata": {
    "id": "step3_5_asr_manual"
   },
   "outputs": [],
   "source": [
    "# @title 4. [CHáº Y SAU KHI SLICING] Táº¡o Phá»¥ Äá» Tiáº¿ng Viá»‡t (ASR)\n",
    "# @markdown **Cháº¡y cell nÃ y sau khi báº¡n Ä‘Ã£ báº¥m nÃºt 'Audio Slicing' trÃªn WebUI xong.**\n",
    "# @markdown NÃ³ sáº½ tá»± Ä‘á»™ng tÃ¬m cÃ¡c file Ä‘Ã£ cáº¯t vÃ  táº¡o file .list tiáº¿ng Viá»‡t chuáº©n.\n",
    "\n",
    "import os\n",
    "\n",
    "# TÃªn thÆ° má»¥c output (máº·c Ä‘á»‹nh WebUI sáº½ táº¡o ra folder nÃ y)\n",
    "# Náº¿u báº¡n Ä‘á»•i tÃªn experiment, hÃ£y Ä‘á»•i cáº£ á»Ÿ Ä‘Ã¢y\n",
    "sliced_audio_folder = f\"output/{exp_name}_sliced\"\n",
    "output_list_file = f\"output/{exp_name}.list\"\n",
    "\n",
    "print(f\"ðŸ“‚ Äang tÃ¬m file Ä‘Ã£ cáº¯t táº¡i: {sliced_audio_folder}\")\n",
    "\n",
    "if not os.path.exists(sliced_audio_folder):\n",
    "    print(\"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c Ä‘Ã£ cáº¯t! Báº¡n Ä‘Ã£ báº¥m nÃºt 'Audio Slicing' trÃªn WebUI chÆ°a?\")\n",
    "else:\n",
    "    print(\"ðŸš€ Äang cháº¡y Whisper (large-v3) Ä‘á»ƒ nháº­n dáº¡ng tiáº¿ng Viá»‡t...\")\n",
    "    # Cháº¡y Whisper ASR\n",
    "    !python tools/asr/faster_whisper_asr.py -i \"{sliced_audio_folder}\" -o \"{output_list_file}\" -s large-v3 -l vi -p float32\n",
    "    \n",
    "    print(f\"\\nâœ… ÄÃ£ xong! File káº¿t quáº£ náº±m táº¡i: {os.path.abspath(output_list_file)}\")\n",
    "    print(\"ðŸ‘‰ BÆ°á»›c tiáº¿p theo: Quay láº¡i WebUI -> Tab '1B-Fine-tuned Training'.\")\n",
    "    print(f\"ðŸ‘‰ Copy Ä‘Æ°á»ng dáº«n nÃ y: '{os.path.abspath(output_list_file)}' dÃ¡n vÃ o Ã´ 'Label file' rá»“i báº¥m 'Format data'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1990781",
   "metadata": {
    "id": "step4_save_model"
   },
   "outputs": [],
   "source": [
    "# @title 5. LÆ°u Model vá» Drive\n",
    "save_to_drive = True # @param {type:\"boolean\"}\n",
    "drive_save_path = f\"/content/drive/MyDrive/GPT_SoVITS_Models/{exp_name}\"\n",
    "\n",
    "if save_to_drive:\n",
    "    from google.colab import drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        drive.mount('/content/drive')\n",
    "    \n",
    "    os.makedirs(drive_save_path, exist_ok=True)\n",
    "    print(f\"Dang luu model {exp_name} vao Drive...\")\n",
    "\n",
    "    !cp GPT_SoVITS/pretrained_models/s2G*.pth \"{drive_save_path}\"/\n",
    "    !cp GPT_SoVITS/pretrained_models/s1bert*.ckpt \"{drive_save_path}\"/\n",
    "    !cp -r GPT_SoVITS/weights/* \"{drive_save_path}\"/ 2>/dev/null || true\n",
    "    !cp -r logs/{exp_name}/* \"{drive_save_path}\"/ 2>/dev/null || true\n",
    "    \n",
    "    print(f\"âœ… Da luu xong tai: {drive_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}